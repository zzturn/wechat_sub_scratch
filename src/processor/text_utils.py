"""
    Created by howie.hu at 2021-12-30.
    Description: é€šç”¨å¤„ç†å‡½æ•°
    Changelog: all notable changes to this file will be documented
"""
import os
import re
import zhipuai
from urllib.parse import urljoin

import html2text
import jieba
import jieba.analyse

from bs4 import BeautifulSoup
from readability import Document

from src.classifier import model_predict_factory
from src.common.remote import get_html_by_requests, send_get_request
from src.config import Config
from src.databases import MongodbManager
from src.utils.log import LOGGER


def ad_marker(
    cos_value: float = 0.6,
    is_force=False,
    basic_filter=None,
    **kwargs,
):
    """å¯¹è®¢é˜…çš„æ–‡ç« è¿›è¡Œå¹¿å‘Šæ ‡è®°

    Args:
        cos_value (str): 0.6
        basic_filter (dict): {} æŸ¥è¯¢æ¡ä»¶
        is_force (bool): æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ¤å†³
    """
    mongo_base = MongodbManager.get_mongo_base(mongodb_config=Config.LL_MONGODB_CONFIG)
    coll = mongo_base.get_collection(coll_name="liuli_articles")
    if is_force:
        query = {}
    else:
        query = {"cos_model": {"$exists": False}}

    query.update(basic_filter or {})

    # æŸ¥æ‰¾æ²¡æœ‰è¢«æ ‡è®°çš„æ–‡ç« ï¼ŒåŸºäºç›¸ä¼¼åº¦æ¨¡å‹è¿›è¡Œåˆ¤æ–­
    for each_data in coll.find(query):
        doc_name = each_data["doc_name"]
        doc_source_name = each_data["doc_source_name"]
        doc_content = each_data["doc_content"]
        doc_keywords = each_data.get("doc_keywords")

        if not doc_keywords:
            keyword_list = extract_keyword_list(doc_content)
            doc_keywords = " ".join(keyword_list)
            each_data["doc_keywords"] = doc_keywords

        # åŸºäºä½™å¼¦ç›¸ä¼¼åº¦
        cos_model_resp = model_predict_factory(
            model_name="cos",
            model_path="",
            input_dict={"text": doc_name + doc_keywords, "cos_value": cos_value},
            # input_dict={"text": doc_name, "cos_value": Config.LL_COS_VALUE},
        ).to_dict()
        each_data["cos_model"] = cos_model_resp
        if cos_model_resp["result"] == 1:
            LOGGER.info(
                f"[{doc_source_name}] {doc_name} è¢«è¯†åˆ«ä¸ºå¹¿å‘Š[{cos_model_resp['probability']}]ï¼Œé“¾æ¥ä¸ºï¼š{each_data['doc_link']}"
            )
        coll.update_one(
            filter={"doc_id": each_data["doc_id"]},
            update={"$set": each_data},
            upsert=True,
        )


def extract_chapters(chapter_url, html):
    """
    é€šç”¨è§£æå°è¯´ç›®å½•
    :param chapter_url: å°è¯´ç›®å½•é¡µurl
    :param res: å½“å‰é¡µé¢html
    :return:
    """
    # å‚è€ƒhttps://greasyfork.org/zh-CN/scripts/292-my-novel-reader
    chapters_reg = (
        r"(<a\s+.*?>.*ç¬¬?\s*[ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹åâ—‹é›¶ç™¾åƒä¸‡äº¿0-9ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼]{1,6}\s*[ç« å›å·èŠ‚æŠ˜ç¯‡å¹•é›†].*?</a>)"
    )
    # è¿™é‡Œä¸èƒ½ä¿è¯è·å–çš„ç« èŠ‚åˆ†å¾—å¾ˆæ¸…æ¥šï¼Œä½†èƒ½ä¿è¯è¿™ä¸€ä¸²stræ˜¯ç« èŠ‚ç›®å½•ã€‚å¯ä»¥åˆ©ç”¨bså®‰å¿ƒæå–a
    chapters_res = re.findall(chapters_reg, str(html), re.I)
    str_chapters_res = "\n".join(chapters_res)
    chapters_res_soup = BeautifulSoup(str_chapters_res, "html5lib")
    all_chapters = []
    for link in chapters_res_soup.find_all("a"):
        each_data = {}
        cur_chapter_url = urljoin(chapter_url, link.get("href")) or ""
        cur_chapter_name = link.text or ""
        if valid_chapter_name(cur_chapter_name):
            each_data["chapter_url"] = cur_chapter_url
            each_data["chapter_name"] = cur_chapter_name
            all_chapters.append(each_data)
    # å»é‡
    all_chapters_sorted = []
    for index, value in enumerate(all_chapters):
        if value not in all_chapters[index + 1:]:
            all_chapters_sorted.append(value)
    return all_chapters_sorted


def extract_core_html(html: str):
    """ä»æ–‡ç« ç±»å‹æå–æ ¸å¿ƒHTML

    Args:
        html (str): raw html
    """
    doc = Document(html)
    return doc.title(), doc.summary()


def extract_keyword_list(url_or_text: str = None):
    """
    è·å–æ–‡æœ¬çš„å…³é”®è¯åˆ—è¡¨
    :param url_or_text:
    :return:
    """
    if url_or_text.startswith("http"):
        resp = send_get_request(url_or_text)
        # TODO å½“textä¸ºç©ºæ—¶å€™éœ€è¦è¿›è¡Œå¤„ç†
        text = html_to_text_h2t(resp.text) if resp else None
    else:
        text = url_or_text
    stop_file_path = os.path.join(
        os.path.join(Config.MODEL_DIR, "data"), "stop_words.txt"
    )
    jieba.analyse.set_stop_words(stop_file_path)
    # keyword_list = jieba.analyse.extract_tags(text, topK=20)
    keyword_list = jieba.analyse.textrank(text, topK=20)

    # from textrank4zh import TextRank4Keyword
    # tr4w = TextRank4Keyword(stop_words_file=stop_file_path)
    # tr4w.analyze(text=text, lower=True, window=2)
    # keyword_list = []
    # for item in tr4w.get_keywords(20, word_min_len=2):
    #     keyword_list.append(item.word)

    return keyword_list


def html_to_text_h2t(html: str):
    """
    ä»htmlæå–æ ¸å¿ƒå†…å®¹text
    :param html:
    :return:
    """
    h = html2text.HTML2Text()
    h.ignore_links = True
    h.bypass_tables = False
    h.unicode_snob = False
    _, summary = extract_core_html(html)
    text = h.handle(summary)
    return text.strip()


def summarize(api_key, model_name, prompt, custom_filter, retry_times=3, **kwargs):
    mongo_base = MongodbManager.get_mongo_base(mongodb_config=Config.LL_MONGODB_CONFIG)
    coll = mongo_base.get_collection(coll_name="liuli_articles")

    query = {}
    query.update(custom_filter or {})

    # æŸ¥æ‰¾æ²¡æœ‰è¢«æ ‡è®°çš„æ–‡ç« ï¼ŒåŸºäºç›¸ä¼¼åº¦æ¨¡å‹è¿›è¡Œåˆ¤æ–­
    for each_data in coll.find(query):
        doc_name = each_data["doc_name"]
        doc_source_name = each_data["doc_source_name"]
        doc_content = each_data["doc_content"]
        format_map = {"title": doc_name, "content": doc_content}
        for i in range(retry_times):
            try:
                response = summarize_content(api_key, prompt.format_map(format_map), model_name, **kwargs)
                if response.code == 200:
                    coll.update_one(
                        filter={"doc_id": each_data["doc_id"]},
                        update={"$set": {"doc_summary": response.data.choices[0].content}},
                    )
                    LOGGER.info(f"ğŸ± æ–‡ç« ->{doc_name} æ‘˜è¦ç¬¬ç”ŸæˆæˆåŠŸ! Cost: {response.data.usage}")
                    break
            except Exception as e:
                LOGGER.error(f"ğŸ˜¿ æ–‡ç« ->{doc_name} æ‘˜è¦ç¬¬ {i + 1} æ¬¡ç”Ÿæˆå¤±è´¥! Error: {str(e)}")
                continue


def summarize_content(api_key, prompt: str, model_name="chatglm_trubo", **kwargs):
    response = zhipuai.model_api.invoke(model=model_name,
                                        prompt=[{"role": "user", "content": prompt}],
                                        temperature=0.2,
                                        top_p=0.7,
                                        **kwargs)
    return response


def str_replace(text: str, before_str: str, after_str: str) -> str:
    """æ–‡æœ¬æ›¿æ¢

    Args:
        text (str): åŸå§‹æ–‡æœ¬
        before_str (str): æ›¿æ¢å‰
        after_str (str): æ›¿æ¢å
    """
    return str(text).replace(before_str, after_str)


def valid_chapter_name(chapter_name):
    """
    åˆ¤æ–­ç›®å½•åç§°æ˜¯å¦åˆç†
    Args:
        chapter_name ([type]): [description]
    """
    for each in ["ç›®å½•"]:
        if each in chapter_name:
            return False
    return True


if __name__ == "__main__":
    # url = "https://mp.weixin.qq.com/s/NKnTiLixjB9h8fSd7Gq8lw"
    url = "https://www.yruan.com/article/38563/28963588.html"
    t_text = get_html_by_requests(url)
    # doc = Document(text)
    # print(doc.title(), doc.short_title(), dir(doc))
    # print(doc.summary())
    res_text = html_to_text_h2t(t_text)
    # print(res_text)
    res = extract_keyword_list(res_text)
    print(res)
